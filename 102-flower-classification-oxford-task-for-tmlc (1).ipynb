{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Importing necessary libraries.\n\n!pip install timm\nfrom glob import glob\nimport cv2\nimport torch\nfrom torch import nn\nimport os\nimport time\nimport random\nimport cv2\nimport torchvision\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nimport timm\n\nimport sklearn\nimport joblib\nfrom sklearn import metrics\nimport cv2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-27T17:31:39.396790Z","iopub.execute_input":"2021-10-27T17:31:39.397103Z","iopub.status.idle":"2021-10-27T17:31:57.220764Z","shell.execute_reply.started":"2021-10-27T17:31:39.397015Z","shell.execute_reply":"2021-10-27T17:31:57.219938Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"The first function is to seed every random phenomenon, so that our experiments and scores are reproducible.<br>\nThe second function takes in image_path and reads the image. As we are using cv2 to read in the image, we also need to convert the image from bgr to rgb. After this we return the Image.<br>\nThe last function is to plot images, for the EDA.","metadata":{}},{"cell_type":"code","source":"'''\nHelper functions\n'''\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = cv2.cvtColor(im_bgr, cv2.COLOR_BGR2RGB )\n    #print(im_rgb)\n    return im_rgb\n\ndef plot_images(df):\n    fig = plt.figure(figsize=(10,10))\n    df = df.sample(9) \n    Images_shape = (512, 512)\n    \n    Image_names = list(df['image_id'])\n    Image_labels = list(df['label'])\n    Path_to_image = '../input/oxford102class/oxford-102-flowers/'\n    Image_path = [os.path.join(Path_to_image,Image_name) for Image_name in Image_names]\n    for i in range(9):\n        plt.subplot(3,3,i+1)\n        img = get_img(Image_path[i-1])\n        img = cv2.resize(img, Images_shape)\n        plt.gca().axes.get_xaxis().set_visible(False)\n        plt.gca().get_yaxis().set_visible(False)\n        plt.imshow(img)\n        plt.title(Image_labels[i-1])","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:40:39.529216Z","iopub.execute_input":"2021-10-27T17:40:39.529503Z","iopub.status.idle":"2021-10-27T17:40:39.540819Z","shell.execute_reply.started":"2021-10-27T17:40:39.529477Z","shell.execute_reply":"2021-10-27T17:40:39.539889Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"You did not give any specific instruction. So this notebook is made with the assumption that you want me to use train, and valid for training and validitating the model. And the test to test the models performance.","metadata":{}},{"cell_type":"code","source":"#Reading in the 3 csv files:- train, valid, test\n\nBase_Dir = '../input/oxford102class/oxford-102-flowers/'\ntrain = pd.read_csv(os.path.join(Base_Dir,'train.txt'), sep = ' ', header=None)\ntest = pd.read_csv(os.path.join(Base_Dir,'test.txt'), sep = ' ', header=None)\nvalid = pd.read_csv(os.path.join(Base_Dir,'valid.txt'), sep = ' ', header=None)\ncolumn_names = [\"image_id\", \"label\"]\ntrain.columns = column_names\ntest.columns = column_names\nvalid.columns = column_names","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:31:57.222898Z","iopub.execute_input":"2021-10-27T17:31:57.223201Z","iopub.status.idle":"2021-10-27T17:31:57.275133Z","shell.execute_reply.started":"2021-10-27T17:31:57.223163Z","shell.execute_reply":"2021-10-27T17:31:57.274279Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"print(f'No of train images: {len(train)}')\nprint(f'No of valid images: {len(valid)}')\nprint(f'No of test images: {len(test)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:31:57.276451Z","iopub.execute_input":"2021-10-27T17:31:57.277205Z","iopub.status.idle":"2021-10-27T17:31:57.283524Z","shell.execute_reply.started":"2021-10-27T17:31:57.277167Z","shell.execute_reply":"2021-10-27T17:31:57.282625Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Visualizing Images","metadata":{}},{"cell_type":"code","source":"plot_images(train)","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:40:41.780493Z","iopub.execute_input":"2021-10-27T17:40:41.781213Z","iopub.status.idle":"2021-10-27T17:40:42.923455Z","shell.execute_reply.started":"2021-10-27T17:40:41.781167Z","shell.execute_reply":"2021-10-27T17:40:42.922565Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"plot_images(valid)","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:40:46.760796Z","iopub.execute_input":"2021-10-27T17:40:46.761101Z","iopub.status.idle":"2021-10-27T17:40:47.943797Z","shell.execute_reply.started":"2021-10-27T17:40:46.761069Z","shell.execute_reply":"2021-10-27T17:40:47.943004Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"plot_images(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:40:49.667304Z","iopub.execute_input":"2021-10-27T17:40:49.667556Z","iopub.status.idle":"2021-10-27T17:40:50.867493Z","shell.execute_reply.started":"2021-10-27T17:40:49.667526Z","shell.execute_reply":"2021-10-27T17:40:50.866645Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"Creating the config dict.<br>\nThe batch size used here is 64, after this the cuda memory of torch was running out.","metadata":{}},{"cell_type":"code","source":"CFG = {\n\n    'seed': 719,\n    'model_arch': 'tf_efficientnet_b4_ns',\n    'img_size': 256,\n    'epochs': 80,\n    'train_bs': 64,\n    'valid_bs': 64,\n    'T_0': 10,\n    'lr': 1e-4,\n    'min_lr': 1e-6,\n    'weight_decay':1e-6,\n    'num_workers': 4,\n    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n    'device': 'cuda:0'\n#     'device': 'cpu'\n}","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:42:05.340214Z","iopub.execute_input":"2021-10-27T16:42:05.341380Z","iopub.status.idle":"2021-10-27T16:42:05.348764Z","shell.execute_reply.started":"2021-10-27T16:42:05.341260Z","shell.execute_reply":"2021-10-27T16:42:05.347668Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Now we create the Dataset.<br>\nThis class will return the Images based on the index. We will those imageds to the dataloader.","metadata":{}},{"cell_type":"code","source":"#Creating the Image Dataset.\n\nclass FlowerDataset(Dataset):\n    def __init__(self, df, data_root, \n                 transforms=None, \n                 output_label=True,\n                ):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        \n        self.output_label = output_label\n        \n        if output_label == True:\n            self.labels = self.df['label'].values\n            \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        \n        # get labels\n        if self.output_label:\n            target = self.labels[index]\n          \n        img  = get_img(\"{}/{}\".format(self.data_root, self.df.loc[index]['image_id']))\n\n        if self.transforms:\n            img = self.transforms(image=img)['image']\n        \n        \n        if self.output_label == True:\n            return img, target\n        else:\n            return img","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:42:05.362728Z","iopub.execute_input":"2021-10-27T16:42:05.363378Z","iopub.status.idle":"2021-10-27T16:42:05.376809Z","shell.execute_reply.started":"2021-10-27T16:42:05.363331Z","shell.execute_reply":"2021-10-27T16:42:05.375842Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"I am using the Albumentations library for the purpose of Image Augmenations. Here I am using a standard set of Augmentations, that I saw used in an flower classification challenge. I did not experiment a lot with this due to having less time.","metadata":{}},{"cell_type":"code","source":"#Image Augmentations \nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_train_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            CoarseDropout(p=0.5),\n            Cutout(p=0.5),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n  \n        \ndef get_valid_transforms():\n    return Compose([\n            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:42:05.378452Z","iopub.execute_input":"2021-10-27T16:42:05.379682Z","iopub.status.idle":"2021-10-27T16:42:05.395073Z","shell.execute_reply.started":"2021-10-27T16:42:05.379572Z","shell.execute_reply":"2021-10-27T16:42:05.393660Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Now we create the Model. Here I am using a pretrained Effecinet-b4 model trained on ImageNet weights. I experimented with few architectures, from resnet18 to efficientnet-b6 model. Efficientnet-b4 gave me the best accuracy, before this the models are simpler, and after this Efficientnet-b5 and b6 were starting to overfit.<br>\n","metadata":{}},{"cell_type":"code","source":"class FlowerImgClassifier(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_arch, pretrained=pretrained)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(n_features, n_class)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:42:05.396925Z","iopub.execute_input":"2021-10-27T16:42:05.397697Z","iopub.status.idle":"2021-10-27T16:42:05.408607Z","shell.execute_reply.started":"2021-10-27T16:42:05.397645Z","shell.execute_reply":"2021-10-27T16:42:05.407521Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Now we create the dataloader. You usually provide the Dataset to model in batches for training. This is done for many reasons, one being memory concerns.<br>\nWe also create the training and validation loop. \n- We are using autocast from torch.cuda.amp for mixed precision training. Thus giving us a significant boost","metadata":{}},{"cell_type":"code","source":"def prepare_dataloader(train_df, valid_df, data_root='../input/oxford102class/oxford-102-flowers/jpg/'):\n    \n    from catalyst.data.sampler import BalanceClassSampler\n        \n    train_ds = FlowerDataset(train_df, data_root, transforms=get_train_transforms(), output_label=True)\n    valid_ds = FlowerDataset(valid_df, data_root, transforms=get_valid_transforms(), output_label=True)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_ds,\n        batch_size=CFG['train_bs'],\n        pin_memory=False,\n        drop_last=False,\n        shuffle=True,        \n        num_workers=CFG['num_workers'],\n        #sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"downsampling\")\n    )\n    val_loader = torch.utils.data.DataLoader(\n        valid_ds, \n        batch_size=CFG['valid_bs'],\n        num_workers=CFG['num_workers'],\n        shuffle=False,\n        pin_memory=False,\n    )\n    return train_loader, val_loader\n\ndef train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None, schd_batch_update=False):\n    model.train()\n\n    t = time.time()\n    running_loss = None\n\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for step, (imgs, image_labels) in pbar:\n        imgs = imgs.to(device).float()\n        image_labels = image_labels.to(device).long()\n\n        #print(image_labels.shape, exam_label.shape)\n        with autocast():\n            image_preds = model(imgs)   #output = model(input)\n            #print(image_preds.shape, exam_pred.shape)\n\n            loss = loss_fn(image_preds, image_labels)\n            \n            scaler.scale(loss).backward()\n\n            if running_loss is None:\n                running_loss = loss.item()\n            else:\n                running_loss = running_loss * .99 + loss.item() * .01\n\n            if ((step + 1) %  CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad() \n                \n                if scheduler is not None and schd_batch_update:\n                    scheduler.step()\n\n            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n                description = f'epoch {epoch} loss: {running_loss:.4f}'\n                \n                pbar.set_description(description)\n                \n    if scheduler is not None and not schd_batch_update:\n        scheduler.step()\n        \ndef valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False):\n    model.eval()\n\n    t = time.time()\n    loss_sum = 0\n    sample_num = 0\n    image_preds_all = []\n    image_targets_all = []\n    \n    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n    for step, (imgs, image_labels) in pbar:\n        imgs = imgs.to(device).float()\n        image_labels = image_labels.to(device).long()\n        \n        image_preds = model(imgs)   #output = model(input)\n        #print(image_preds.shape, exam_pred.shape)\n        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n        image_targets_all += [image_labels.detach().cpu().numpy()]\n        \n        loss = loss_fn(image_preds, image_labels)\n        \n        loss_sum += loss.item()*image_labels.shape[0]\n        sample_num += image_labels.shape[0]  \n\n        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n            pbar.set_description(description)\n    \n    image_preds_all = np.concatenate(image_preds_all)\n    image_targets_all = np.concatenate(image_targets_all)\n    print('validation multi-class accuracy = {:.4f}'.format((image_preds_all==image_targets_all).mean()))\n    \n    if scheduler is not None:\n        if schd_loss_update:\n            scheduler.step(loss_sum/sample_num)\n        else:\n            scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:42:05.410992Z","iopub.execute_input":"2021-10-27T16:42:05.411391Z","iopub.status.idle":"2021-10-27T16:42:05.438901Z","shell.execute_reply.started":"2021-10-27T16:42:05.411345Z","shell.execute_reply":"2021-10-27T16:42:05.437856Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"This is the training loop. We create the train and valid dataloader, the model, optimizer, scheduler etc. Then we start the training process. After every epoch of training we store the model state dict.","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n     # for training only, need nightly build pytorch\n\n    seed_everything(CFG['seed'])\n    \n    train_loader, val_loader = prepare_dataloader(train, valid, data_root='../input/oxford102class/oxford-102-flowers/')\n\n    device = torch.device(CFG['device'])\n        \n    model = FlowerImgClassifier(CFG['model_arch'], train.label.nunique(), pretrained=True).to(device)\n    scaler = GradScaler()   \n    optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=CFG['epochs']-1)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n    #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=25, \n    #                                                max_lr=CFG['lr'], epochs=CFG['epochs'], steps_per_epoch=len(train_loader))\n        \n    loss_tr = nn.CrossEntropyLoss().to(device) #MyCrossEntropyLoss().to(device)\n    loss_fn = nn.CrossEntropyLoss().to(device)\n        \n    for epoch in range(CFG['epochs']):\n        train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, scheduler=scheduler, schd_batch_update=False)\n\n        with torch.no_grad():\n                valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False)\n\n        torch.save(model.state_dict(),'{}_epoch_{}'.format(CFG['model_arch'], epoch))\n            \n    #torch.save(model.cnn_model.state_dict(),'{}/cnn_model_fold_{}_{}'.format(CFG['model_path'], fold, CFG['tag']))\n    del model, optimizer, train_loader, val_loader, scaler, scheduler\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:42:05.443253Z","iopub.execute_input":"2021-10-27T16:42:05.443609Z","iopub.status.idle":"2021-10-27T17:17:11.182657Z","shell.execute_reply.started":"2021-10-27T16:42:05.443541Z","shell.execute_reply":"2021-10-27T17:17:11.181559Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"The training is over. The best accuracy we got was on epoch 75","metadata":{}},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"I made sure to only use the test dataset once. At the very end. After all the experiments, the model that performed the best on the validation dataset, was used to predict the test images labels.\n<br>We write the inference one epoch function. ","metadata":{}},{"cell_type":"code","source":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        image_preds = model(imgs)   #output = model(input)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    \n    image_preds_all = np.concatenate(image_preds_all)\n    return image_preds_all","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:24:39.470282Z","iopub.execute_input":"2021-10-27T17:24:39.470627Z","iopub.status.idle":"2021-10-27T17:24:39.478263Z","shell.execute_reply.started":"2021-10-27T17:24:39.470562Z","shell.execute_reply":"2021-10-27T17:24:39.476999Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"We create the test_dataset and the test_dataloader. Then we load our previously trained model and predict for the test images.","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\nif __name__ == '__main__':\n     # for training only, need nightly build pytorch\n\n    seed_everything(CFG['seed'])\n     \n    test_ds = FlowerDataset(test, '../input/oxford102class/oxford-102-flowers/', transforms=get_valid_transforms(), output_label=False)\n        \n    tst_loader = torch.utils.data.DataLoader(\n            test_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=False,\n        )\n\n    device = torch.device(CFG['device'])\n    model = FlowerImgClassifier(CFG['model_arch'], train.label.nunique()).to(device)\n        \n    tst_preds = []\n        \n    model.load_state_dict(torch.load('./tf_efficientnet_b4_ns_epoch_75'))\n    with torch.no_grad():\n                tst_preds += [inference_one_epoch(model, tst_loader, device)]","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:20:41.066648Z","iopub.execute_input":"2021-10-27T17:20:41.067765Z","iopub.status.idle":"2021-10-27T17:21:51.261748Z","shell.execute_reply.started":"2021-10-27T17:20:41.067713Z","shell.execute_reply":"2021-10-27T17:21:51.260645Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"np.array(tst_preds).shape","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:21:51.267573Z","iopub.execute_input":"2021-10-27T17:21:51.270525Z","iopub.status.idle":"2021-10-27T17:21:51.286593Z","shell.execute_reply.started":"2021-10-27T17:21:51.270458Z","shell.execute_reply":"2021-10-27T17:21:51.285281Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"test_preds = np.argmax(tst_preds, axis=2)","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:21:51.288768Z","iopub.execute_input":"2021-10-27T17:21:51.289424Z","iopub.status.idle":"2021-10-27T17:21:51.296612Z","shell.execute_reply.started":"2021-10-27T17:21:51.289375Z","shell.execute_reply":"2021-10-27T17:21:51.295350Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"test_preds.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:21:51.300210Z","iopub.execute_input":"2021-10-27T17:21:51.301011Z","iopub.status.idle":"2021-10-27T17:21:51.309351Z","shell.execute_reply.started":"2021-10-27T17:21:51.300958Z","shell.execute_reply":"2021-10-27T17:21:51.308054Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"test_preds","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:21:51.311691Z","iopub.execute_input":"2021-10-27T17:21:51.312452Z","iopub.status.idle":"2021-10-27T17:21:51.321748Z","shell.execute_reply.started":"2021-10-27T17:21:51.312405Z","shell.execute_reply":"2021-10-27T17:21:51.320501Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"test_preds.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:21:51.324743Z","iopub.execute_input":"2021-10-27T17:21:51.327150Z","iopub.status.idle":"2021-10-27T17:21:51.338455Z","shell.execute_reply.started":"2021-10-27T17:21:51.327103Z","shell.execute_reply":"2021-10-27T17:21:51.337407Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"print('test accuracy = {:.5f}'.format((test.label.values==test_preds).mean()))\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-27T17:22:34.433439Z","iopub.execute_input":"2021-10-27T17:22:34.434166Z","iopub.status.idle":"2021-10-27T17:22:34.440239Z","shell.execute_reply.started":"2021-10-27T17:22:34.434123Z","shell.execute_reply":"2021-10-27T17:22:34.439180Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"The final test accuracy is 0.90259","metadata":{}}]}